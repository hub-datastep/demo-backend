services:
  backend:
    build:
      context: .
      dockerfile: ./Dockerfile
    restart: always
    command: bash -c "
      cd /app/src
      && alembic upgrade head
      && python3 /app/src/app.py"
    ports:
      - "8090:8080"
    volumes:
      - ./src:/app/src
      - ./.env:/app/.env
      # - ./.env.development:/app/.env.development
      # - ./.env.production:/app/.env.production
      - backend:${DATA_FOLDER_PATH}
    env_file:
      - .env
      # - .env.${ENVIRONMENT}
    depends_on:
      - db
  
  orders-tasks-tracking:
    image: datastep-backend
    restart: always
    command: python3 /app/src/orders_tasks_tracking.py
    volumes:
      - ./src:/app/src
      - ./.env:/app/.env
      # - ./.env.development:/app/.env.development
      # - ./.env.production:/app/.env.production
      - backend:${DATA_FOLDER_PATH}
    env_file:
      - .env
      # - .env.${ENVIRONMENT}
    depends_on:
      - db

  order-notifications-consumer:
    image: datastep-backend
    restart: always
    command: python3 /app/src/kafka/consumer/order_notifications.py
    volumes:
      - ./src:/app/src
      - ./.env:/app/.env
      # - ./.env.development:/app/.env.development
      # - ./.env.production:/app/.env.production
      - backend:${DATA_FOLDER_PATH}
    env_file:
      - .env
      # - .env.${ENVIRONMENT}
    depends_on:
      - backend
      - kafka

  mapping-with-parsing-consumer:
    image: datastep-backend
    restart: always
    command: python3 /app/src/kafka/consumer/mapping_with_parsing.py
    volumes:
      - ./src:/app/src
      - ./.env:/app/.env
      # - ./.env.development:/app/.env.development
      # - ./.env.production:/app/.env.production
      - backend:${DATA_FOLDER_PATH}
    env_file:
      - .env
      # - .env.${ENVIRONMENT}
    depends_on:
      - backend
      - kafka

  nsi-fetch-consumer:
    image: datastep-backend
    restart: always
    command: python3 /app/src/kafka/consumer/nsi_fetch.py
    volumes:
      - ./src:/app/src
      - ./.env:/app/.env
      # - ./.env.development:/app/.env.development
      # - ./.env.production:/app/.env.production
      - backend:${DATA_FOLDER_PATH}
    env_file:
      - .env
      # - .env.${ENVIRONMENT}
    depends_on:
      - backend
      - kafka

  redis-server:
    image: redis/redis-stack-server
    restart: always
    command: bash -c "redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}"
    ports:
      - "6389:6379"
    volumes:
      - redis-server:${DATA_FOLDER_PATH}
    env_file:
      - .env
      # - .env.${ENVIRONMENT}

  rq-worker:
    image: datastep-backend
    restart: always
    command: rq worker --path /app/src --url redis://:${REDIS_PASSWORD}@${REDIS_HOST}:${REDIS_PORT} mapping_nomenclatures sync_nomenclatures retrain_classifier classification documents
    volumes:
      - ./src:/app/src
      - ./.env:/app/.env
      # - ./.env.development:/app/.env.development
      # - ./.env.production:/app/.env.production
      - backend:${DATA_FOLDER_PATH}
    env_file:
      - .env
      # - .env.${ENVIRONMENT}
    depends_on:
      - backend
      - redis-server

  rq-dashboard:
    image: eoranged/rq-dashboard
    ports:
      - "9181:9181"
    environment:
      - RQ_DASHBOARD_REDIS_URL=redis://:${REDIS_PASSWORD}@${REDIS_HOST}:${REDIS_PORT}
    env_file:
      - .env
      # - .env.${ENVIRONMENT}
    depends_on:
      - redis-server

  db:
    image: postgres
    restart: always
    ports:
      - "5442:5432"
    volumes:
      - db:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - PGPOOL_MAX_CONNECTIONS=1000
      - PGPOOL_NUM_INIT_CHILDREN=250
    env_file:
      - .env
      # - .env.${ENVIRONMENT}

  dbeaver:
    image: dbeaver/cloudbeaver
    restart: always
    ports:
      - "8988:8978"
    volumes:
      - dbeaver:/opt/cloudbeaver/workspace
    depends_on:
      - db

  volumes-viewer:
    image: hurlenko/filebrowser
    restart: always
    ports:
      - "9080:8080"
    volumes:
      - backend:/data
      - volumes-viewer:/config
      # Just to see .env in files viewer
      - ./.env:/data/.env-file

  chroma-db:
    image: chromadb/chroma
    restart: always
    ports:
      - "8011:8000"
    volumes:
      - chroma-db:/chroma/chroma/
    environment:
      - IS_PERSISTENT=TRUE

  kafka:
    image: bitnami/kafka
    restart: always
    # ports:
    #   - "9092:9092"
    volumes:
      - kafka:/bitnami/kafka
    environment:
      # # KRaft
      # - KAFKA_CFG_NODE_ID=0
      # - KAFKA_CFG_PROCESS_ROLES=broker,controller
      # - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      # # Listeners
      # # - KAFKA_CFG_LISTENERS=BROKER://:9092,CONTROLLER://:9093
      # # - KAFKA_CFG_ADVERTISED_LISTENERS=BROKER://kafka:9092
      # # - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:SASL_PLAINTEXT,BROKER:SASL_PLAINTEXT
      # - KAFKA_CFG_LISTENERS=BROKER://:9092,CONTROLLER://:9093
      # - KAFKA_CFG_ADVERTISED_LISTENERS=BROKER://kafka:9092
      # - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT
      # # Names
      # - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      # - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=BROKER
      # # SASL
      # # - KAFKA_CFG_SASL_ENABLED_MECHANISMS=PLAIN
      # # - KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL=PLAIN
      # # - KAFKA_CFG_SASL_MECHANISM_CONTROLLER_PROTOCOL=PLAIN
      # # - KAFKA_CLIENT_USERS=${KAFKA_USERNAME}
      # # - KAFKA_CLIENT_PASSWORDS=${KAFKA_PASSWORD}
      # - KAFKA_CFG_SUPER_USERS=User:${KAFKA_USERNAME}
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9092
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - ALLOW_PLAINTEXT_LISTENER=yes

  kafka-ui:
    image: provectuslabs/kafka-ui
    ports:
      - "9090:8080"
    volumes:
      - kafka-ui:/etc/kafkaui
    environment:
      - KAFKA_CLUSTERS_0_NAME=${KAFKA_UI_CLUSTER_0_NAME}
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:9092
      - AUTH_TYPE=LOGIN_FORM
      - SPRING_SECURITY_USER_NAME=${KAFKA_UI_USERNAME}
      - SPRING_SECURITY_USER_PASSWORD=${KAFKA_UI_PASSWORD}
    env_file:
      - .env
      # - .env.${ENVIRONMENT}
    depends_on:
      - kafka

  logs-viewer:
    image: amir20/dozzle
    restart: always
    ports:
      - "8070:8080"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./logs-viewer-users.yml:/data/users.yml
    environment:
      DOZZLE_AUTH_PROVIDER: simple
      DOZZLE_ENABLE_ACTIONS: "true"


volumes:
  backend:
  db:
  dbeaver:
  redis-server:
  chroma-db:
    driver: local
  volumes-viewer:
  kafka:
  kafka-ui:
